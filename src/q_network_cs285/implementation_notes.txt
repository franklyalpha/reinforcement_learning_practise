always realize that for any reinforcement learning algorithms, the procedures would be performed as follows:
1: generate samples
2: fit the model for those samples; 
3: update models.

And above is a recursive process. 

# note to the package gymnasium: 
- this is a forked package from OpenAI's gym package (which was also being installed on my computer, but due to it's using mujoco-py package, and I was facing troubles installing that, I quit using "gym"), which requires "mujoco" package, and seems like the environment to be working properly. 
- https://pypi.org/project/gymnasium/ provides an easy-to-use example for acquiring samples from simulated environment. 

# q_network details:
need a replay buffer.
for each epoch:
    gather a set of datapoints using latest policy, and store them in replay buffer.
    perform "batch_size" many updates as follows:
        take "trajectory_time" many datapoints stored in replay buffer,
        and perform gradient update on policy network.

realizing that this time, the network would still take in states as inputs, but will output
values for each actions (perhaps need to be careful of how rewards are being defined in this case)
then the actions yielding the highest value would be taken. (argmax)